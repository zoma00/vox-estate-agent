
why you select the cli audio simple .py file. 
it is not integrated with open ai at all.
** and we should avoide the cli at all since our frontend integrated with backend using the :
backend/realestate_agent/app/agent_pipeline.py


```python```

from fastapi import UploadFile
from typing import Optional
from tts.tts_coqui import generate_tts_audio as coqui_generate_tts_audio
import openai  # Make sure openai is installed: pip install openai
import os
import dotenv
dotenv.load_dotenv()  # Loads variables from .env if present
print("Loaded OpenAI key:", os.getenv("OPENAI_API_KEY"))

# Placeholder for AI response generation (e.g., OpenAI GPT)
def generate_ai_response(user_text: str) -> str:
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": user_text}]
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"OpenAI error: {e}")
        return "Sorry, I couldn't generate a response."


# Placeholder for TTS audio generation
def generate_tts_audio(text: str, voice: Optional[str] = None) -> str:
    # Use the real Coqui TTS implementation
    output_path = "tts_output.wav"
    try:
        return coqui_generate_tts_audio(text, output_path=output_path)
    except Exception as e:
        # Log or handle error as needed
        print(f"TTS generation failed: {e}")
        return ""

# Placeholder for avatar animation (talking head video)
def animate_avatar(image_path: str, audio_path: str) -> str:
    # TODO: Integrate with SadTalker/Wav2Lip, return path to video file
    return "path/to/generated_video.mp4"

```

** And we have main.py file:
------------------------------
# Entry point for FastAPI app
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import FileResponse
from fastapi import Form
from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from app.agent_pipeline import generate_ai_response


from app.agent_pipeline import generate_ai_response, generate_tts_audio, animate_avatar

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Or specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    return {"message": "Real Estate AI Agent API running."}


@app.post("/agent/respond")
def agent_respond(user_text: str):
    ai_response = generate_ai_response(user_text)
    return {"response": ai_response}


@app.post("/agent/tts")
def agent_tts(text:str = Form(...)):
    audio_path = generate_tts_audio(text)
    if not audio_path:
        return {"error": "TTS generation failed."}
    return FileResponse(audio_path, media_type="audio/wav", filename="tts_output.wav")


@app.post("/agent/animate")
def agent_animate(image: UploadFile = File(...), audio: UploadFile = File(...)):
    # In a real implementation, save files and pass paths to animate_avatar
    image_path = f"/tmp/{image.filename}"
    audio_path = f"/tmp/{audio.filename}"
    with open(image_path, "wb") as img_f:
        img_f.write(image.file.read())
    with open(audio_path, "wb") as aud_f:
        aud_f.write(audio.file.read())
    video_path = animate_avatar(image_path, audio_path)
    return {"video_path": video_path}

@app.post("/api/openai_chat")
async def openai_chat(text: str = Body(..., embed=True)):
    response = generate_ai_response(text)  # Use your real AI logic here
    return {"response": response}


@app.post("/api/tts")
async def tts(text: str = Body(..., embed=True)):
    audio_path = generate_tts_audio(text)  # Uses your agent_pipeline.py logic
    if not audio_path:
        return {"error": "TTS generation failed."}
    return FileResponse(audio_path, media_type="audio/wav", filename="tts_output.wav")

** So use them for the Parse the AI response for URLs using regex.




Answer:
---------
























To open URLs in the web browser based on the chat answer, you can:

1. Parse the AI response for URLs using a regular expression.
2. Use Pythonâ€™s `webbrowser` module to open the URL.

**Example:**
```python
import re
import webbrowser

# Suppose ai_text is the response from OpenAI
ai_text = "You can find more info at https://www.example.com."

# Find URLs in the response
urls = re.findall(r'https?://[^\s]+', ai_text)
for url in urls:
    print(f"Opening: {url}")
    webbrowser.open(url)
```

**How to integrate:**
- After printing the AI response, check for URLs and open them automatically or prompt the user to confirm before opening.

Let me know if you want this added to your CLI script!

#######################################################


You can add this feature to your CLI script to automatically detect and 
open URLs from the AI response.

**How to integrate into your CLI:**
1. After receiving the AI response, use a regex to find URLs.
2. Use `webbrowser.open(url)` to open each URL.

**Example integration for your CLI:**
```python
import re
import webbrowser

# ...existing code to get ai_text from OpenAI...

print(f"AI response: {ai_text}")

# Find and open URLs
urls = re.findall(r'https?://[^\s]+', ai_text)
for url in urls:
    print(f"Opening: {url}")
    webbrowser.open(url)
```

**Optional:**  
Prompt the user before opening each URL:
```python
for url in urls:
    confirm = input(f"Open {url}? (y/n): ")
    if confirm.lower() == 'y':
        webbrowser.open(url)
```

Let me know if you want this added to a specific CLI file or need a full 
code update!