
agent/backend/realestate_agent$ python -m uvicorn app.main:app --host 0.0.0.0 --port 8000
INFO:     Started server process [133139]
INFO:     Waiting for application startup.
2025-09-17 01:38:00,390 - app.main - INFO - Application startup: Created necessary directories
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-09-17 01:38:07,005 - app.agent_pipeline - INFO - Sending request to OpenAI with model: gpt-3.5-turbo
2025-09-17 01:38:09,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-17 01:38:09,809 - app.agent_pipeline - INFO - Successfully generated response with 51 characters
2025-09-17 01:38:34,656 - app.agent_pipeline - INFO - Generating TTS audio for 51 characters in en
 > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.
 > Using model: xtts
 ** it takes too much time and in the end raise error 500:
 INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-09-17 01:38:07,005 - app.agent_pipeline - INFO - Sending request to OpenAI with model: gpt-3.5-turbo
2025-09-17 01:38:09,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-17 01:38:09,809 - app.agent_pipeline - INFO - Successfully generated response with 51 characters
2025-09-17 01:38:34,656 - app.agent_pipeline - INFO - Generating TTS audio for 51 characters in en
 > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.
 > Using model: xtts
2025-09-17 01:39:03,432 - app.agent_pipeline - ERROR - Failed to generate TTS audio: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. 
        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
        WeightsUnpickler error: Unsupported global: GLOBAL TTS.tts.models.xtts.XttsAudioConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TTS.tts.models.xtts.XttsAudioConfig])` or the `torch.serialization.safe_globals([TTS.tts.models.xtts.XttsAudioConfig])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
Traceback (most recent call last):
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/app/agent_pipeline.py", line 149, in generate_tts_audio
    audio_path = coqui_generate_tts_audio(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/tts/tts_coqui.py", line 19, in generate_tts_audio
    tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False, gpu=False)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/api.py", line 74, in __init__
    self.load_tts_model_by_name(model_name, gpu)
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/api.py", line 177, in load_tts_model_by_name
    self.synthesizer = Synthesizer(
                       ^^^^^^^^^^^^
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/utils/synthesizer.py", line 109, in __init__
    self._load_tts_from_dir(model_dir, use_cuda)
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/utils/synthesizer.py", line 164, in _load_tts_from_dir
    self.tts_model.load_checkpoint(config, checkpoint_dir=model_dir, eval=True)
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/tts/models/xtts.py", line 771, in load_checkpoint
    checkpoint = self.get_compatible_checkpoint_state_dict(model_path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/tts/models/xtts.py", line 714, in get_compatible_checkpoint_state_dict
    checkpoint = load_fsspec(model_path, map_location=torch.device("cpu"))["model"]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/TTS/utils/io.py", line 54, in load_fsspec
    return torch.load(f, map_location=map_location, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hazem-elbatawy/Downloads/vox-estate-agent/backend/realestate_agent/venv/lib/python3.11/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. 
        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
        WeightsUnpickler error: Unsupported global: GLOBAL TTS.tts.models.xtts.XttsAudioConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TTS.tts.models.xtts.XttsAudioConfig])` or the `torch.serialization.safe_globals([TTS.tts.models.xtts.XttsAudioConfig])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
INFO:     127.0
** This is because we changed the head of the coqui tts audio cod


Answer:
=------------

You are correct—the error is because PyTorch 2.6+ now requires you to allowlist custom classes when loading model weights. For XTTS v2, you must add the following line at the very top of your TTS code (before importing and using Coqui TTS):

```python
import torch
torch.serialization.add_safe_globals([__import__('TTS.tts.models.xtts').tts.models.xtts.XttsAudioConfig])
```

This will allow PyTorch to safely unpickle the XTTS audio config and load the model without error 500.

**How to fix:**
1. Add the above line to your `tts_coqui.py` (or wherever you load the XTTS model).
2. Restart your backend server.

This should resolve the “Weights only load failed” error and allow TTS generation to work. Let me know if you need the exact code placement or further troubleshooting!




Answer

You need to add both allowlist lines for XTTS v2 to work with PyTorch 2.6+:

Add these at the very top of your TTS script, before importing or using Coqui TTS:

```python
import torch
torch.serialization.add_safe_globals([__import__('TTS.tts.configs.xtts_config').tts.configs.xtts_config.XttsConfig])
torch.serialization.add_safe_globals([__import__('TTS.tts.models.xtts').tts.models.xtts.XttsAudioConfig])
```

The second line is required for the error you’re seeing:
> Unsupported global: GLOBAL TTS.tts.models.xtts.XttsAudioConfig

Restart your backend after adding both lines. This should resolve the error 500 and allow XTTS v2 to load and generate audio.




You need to add one more allowlist line for the new error:

Add this at the top of your TTS script, along with the previous lines:

```python
import torch
torch.serialization.add_safe_globals([__import__('TTS.tts.configs.xtts_config').tts.configs.xtts_config.XttsConfig])
torch.serialization.add_safe_globals([__import__('TTS.tts.models.xtts').tts.models.xtts.XttsAudioConfig])
torch.serialization.add_safe_globals([__import__('TTS.config.shared_configs').config.shared_configs.BaseDatasetConfig])
```

This last line fixes:
> Unsupported global: GLOBAL TTS.config.shared_configs.BaseDatasetConfig

Restart your backend after adding all three lines. This should resolve the error and allow XTTS v2 to load and generate audio.




##################################################



Thanks for the full traceback 🙏 this clears things up.

The key line is:

```
'GPT2InferenceModel' object has no attribute 'generate'
```

This isn’t about your speaker handling anymore — it’s because Hugging Face **Transformers ≥4.50** removed `GenerationMixin` from `PreTrainedModel`.

XTTS v2 internally expects its `GPT2InferenceModel` to still have `.generate()`, but with new Transformers versions it doesn’t. That’s why Coqui TTS is breaking.

---

## 🔧 Fix Options

### 1. **Pin Transformers to a working version**

Coqui XTTS v2 works with `transformers < 4.50`.

Run:

```bash
pip install "transformers<4.50"
```

or if you want a known good version:

```bash
pip install transformers==4.39.3
```

---

### 2. **Patch `GPT2InferenceModel` manually** (if you must use ≥4.50)

Add this patch after your imports:

```python
from transformers.generation.utils import GenerationMixin
from TTS.tts.layers.xtts.gpt import GPT2InferenceModel

if not issubclass(GPT2InferenceModel, GenerationMixin):
    GPT2InferenceModel.__bases__ += (GenerationMixin,)
```

This forces `GPT2InferenceModel` to regain `.generate()`.

---

### 3. **Best practice**

Since Coqui TTS hasn’t caught up with Transformers’ breaking change yet, I recommend **option 1 (pin Transformers)**. It’s stable and doesn’t require hacks.

---

✅ So, in your project, I’d first try:

```bash
pip install transformers==4.39.3
```

Then rerun your script — the `generate` method will exist again.

---

Do you want me to **patch your `tts_coqui.py` automatically** so it detects Transformers ≥4.50 and injects the `GenerationMixin` fix, so you don’t depend on pip pinning?














 > Downloading model to /home/hazem-elbatawy/snap/code/206/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2

 > Model's license - CPML
 > Check https://coqui.ai/cpml.txt for more info.
 > Using model: xtts
TTS generation failed: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. 
        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
        WeightsUnpickler error: Unsupported global: GLOBAL TTS.tts.configs.xtts_config.XttsConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TTS.tts.configs.xtts_config.XttsConfig])` or the `torch.serialization.safe_globals([TTS.tts.configs.xtts_config.XttsConfig])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.


lingual--multi-dataset--xtts_v2$ du -sh .
1.8G   


Your XTTS v2 model download is nearly complete, but you’re seeing a PyTorch 2.6 “weights_only” error when trying to load the model. This is a known issue with recent PyTorch versions and custom checkpoints.

To fix this, you have two safe options:

**Option 1: Allowlist the required class (recommended for trusted sources):**
Add this code before loading the model in your script:
```python
import torch
torch.serialization.add_safe_globals([__import__('TTS.tts.configs.xtts_config').tts.configs.xtts_config.XttsConfig])
```
Then load the model as usual.

**Option 2: Set weights_only=False (less safe, only if you trust the source):**
If you control the model loading, set `weights_only=False` in `torch.load`:
```python
torch.load('model.pth', weights_only=False)
```
But this is only possible if you directly call `torch.load`.

**Summary:**
- For Coqui TTS XTTS v2, add the allowlist code above before running your TTS script.
- This will resolve the “Unsupported global” error and allow the model to load.

Let me know if you want the exact code update for your backend TTS module!


yes 

Answer:
------------

Here’s the updated code for your `tts_coqui.py` to fix the XTTS v2 “weights_only” error in PyTorch 2.6+:

```python
# Basic TTS module using Coqui TTS XTTS v2 (multilingual, supports Arabic)
# Install with: pip install TTS numpy scipy
import torch
# Add this line to allow XTTS config class for safe model loading
torch.serialization.add_safe_globals([__import__('TTS.tts.configs.xtts_config').tts.configs.xtts_config.XttsConfig])

from TTS.api import TTS
import numpy as np
from scipy.io.wavfile import write
from typing import Union, List

def generate_tts_audio(
    text: Union[str, List[str]],
    output_path: str = "tts_output.wav",
    language: str = "en"
) -> str:
    import time
    # Use XTTS v2 (multilingual, supports Arabic)
    tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False, gpu=False)
    if isinstance(text, str):
        texts = [text]
    else:
        texts = list(text)
    # Generate audio for all sentences by looping
    wavs = [tts.tts(sentence, language=language) for sentence in texts]
    # Concatenate all audio arrays
    sample_rate = tts.synthesizer.output_sample_rate
    combined = np.concatenate(wavs)
    # Generate timestamped filename
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_path = f"tts_output_{timestamp}.wav"
    # Normalize audio to -1.0 to 1.0 range before converting to int16
    max_val = np.max(np.abs(combined))
    if max_val > 0:
        combined = combined / max_val
    write(output_path, sample_rate, (combined * 32767).astype(np.int16))
    return output_path

if __name__ == "__main__":
    test_texts = [
        "Hello, this is a test of the Coqui TTS system.",
        "This demonstrates batching multiple sentences.",
        "All sentences will be combined into one audio file.",
        "مرحبا! هذا اختبار للنظام باللغة العربية."  # Arabic test
    ]
    # English example
    audio_file_en = generate_tts_audio(test_texts[:3], language="en")
    print(f"English audio saved to {audio_file_en}")
    # Arabic example
    audio_file_ar = generate_tts_audio(test_texts[3], language="ar")
    print(f"Arabic audio saved to {audio_file_ar}")
```

Just add the `torch.serialization.add_safe_globals(...)` line at the top, and you’ll be able to load XTTS v2 and generate Arabic speech.


Great! Your backend TTS code is now ready to generate Arabic speech using XTTS v2. If you run into any issues or want to add auto language detection, batch processing, or frontend integration, just let me know. You can now demo multilingual TTS (including Arabic) for your clients.




############################################
Finished download   
###########################33333


Yes, this means the XTTS v2 model has finished downloading and is now available for use. 
The license prompt and the successful HTTP 200 responses indicate your backend is running 
and responding.

If you still see the “Weights only load failed” error, make sure you’ve added the following
 line at the top of your TTS script (before importing and using Coqui TTS):

```python
import torch
torch.serialization.add_safe_globals([__import__('TTS.tts.configs.xtts_config').tts.configs.xtts_config.XttsConfig])
```

Once this is in place, your backend should be able to generate TTS (including Arabic) without further errors. You’re ready to demo multilingual TTS!



















Coqui TTS (glow-tts) only supports certain languages and voices. If your TTS model is set to English (e.g., `tts_models/en/ljspeech/glow-tts`), it cannot synthesize Arabic speech.

**How to solve:**
1. **Check available Arabic TTS models:**  
   Run `tts --list_models` in your terminal to see if there are Arabic models available (look for `ar` in the model name).

2. **Install and use an Arabic TTS model:**  
   If available, change your code to use an Arabic model, e.g.:
   ```
   tts --model_name tts_models/ar/your_arabic_model --text "مرحبا!"
   ```
   Or update your Python code to use the Arabic model.

3. **If no Arabic model is available:**  
   - Consider using another TTS library that supports Arabic,
    such as Google Cloud TTS, Azure TTS, or responsive web APIs.
   - You can also use open-source alternatives like 
   [Silero TTS](https://github.com/snakers4/silero-models) which supports Arabic.

**Summary:**  
- Use a TTS model that supports Arabic.
- If Coqui TTS does not have Arabic, switch to a provider or library that does.

Let me know if you want help switching models or integrating another TTS provider!





state_agent$ source venv/bin/activate
(venv) hazem-elbatawy@hazem-elbatawy-Vostro-15-3510:~/Downloads/vox-estate-agent/backend/realestate_agent$ tts --list_models

 Name format: type/language/dataset/model
 1: tts_models/multilingual/multi-dataset/xtts_v2
 2: tts_models/multilingual/multi-dataset/xtts_v1.1
 3: tts_models/multilingual/multi-dataset/your_tts
 4: tts_models/multilingual/multi-dataset/bark
 5: tts_models/bg/cv/vits
 6: tts_models/cs/cv/vits
 7: tts_models/da/cv/vits
 8: tts_models/et/cv/vits
 9: tts_models/ga/cv/vits
 10: tts_models/en/ek1/tacotron2
 11: tts_models/en/ljspeech/tacotron2-DDC [already downloaded]
 12: tts_models/en/ljspeech/tacotron2-DDC_ph
 13: tts_models/en/ljspeech/glow-tts [already downloaded]
 14: tts_models/en/ljspeech/speedy-speech
 15: tts_models/en/ljspeech/tacotron2-DCA
 16: tts_models/en/ljspeech/vits
 17: tts_models/en/ljspeech/vits--neon
 18: tts_models/en/ljspeech/fast_pitch
 19: tts_models/en/ljspeech/overflow
 20: tts_models/en/ljspeech/neural_hmm
 21: tts_models/en/vctk/vits
 22: tts_models/en/vctk/fast_pitch
 23: tts_models/en/sam/tacotron-DDC
 24: tts_models/en/blizzard2013/capacitron-t2-c50
 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2
 26: tts_models/en/multi-dataset/tortoise-v2
 27: tts_models/en/jenny/jenny
 28: tts_models/es/mai/tacotron2-DDC
 29: tts_models/es/css10/vits
 30: tts_models/fr/mai/tacotron2-DDC
 31: tts_models/fr/css10/vits
 32: tts_models/uk/mai/glow-tts
 33: tts_models/uk/mai/vits
 34: tts_models/zh-CN/baker/tacotron2-DDC-GST
 35: tts_models/nl/mai/tacotron2-DDC
 36: tts_models/nl/css10/vits
 37: tts_models/de/thorsten/tacotron2-DCA
 38: tts_models/de/thorsten/vits
 39: tts_models/de/thorsten/tacotron2-DDC
 40: tts_models/de/css10/vits-neon
 41: tts_models/ja/kokoro/tacotron2-DDC
 42: tts_models/tr/common-voice/glow-tts
 43: tts_models/it/mai_female/glow-tts
 44: tts_models/it/mai_female/vits
 45: tts_models/it/mai_male/glow-tts
 46: tts_models/it/mai_male/vits
 47: tts_models/ewe/openbible/vits
 48: tts_models/hau/openbible/vits
 49: tts_models/lin/openbible/vits
 50: tts_models/tw_akuapem/openbible/vits
 51: tts_models/tw_asante/openbible/vits
 52: tts_models/yor/openbible/vits
 53: tts_models/hu/css10/vits
 54: tts_models/el/cv/vits
 55: tts_models/fi/css10/vits
 56: tts_models/hr/cv/vits
 57: tts_models/lt/cv/vits
 58: tts_models/lv/cv/vits
 59: tts_models/mt/cv/vits
 60: tts_models/pl/mai_female/vits
 61: tts_models/pt/cv/vits
 62: tts_models/ro/cv/vits
 63: tts_models/sk/cv/vits
 64: tts_models/sl/cv/vits
 65: tts_models/sv/cv/vits
 66: tts_models/ca/custom/vits
 67: tts_models/fa/custom/glow-tts
 68: tts_models/bn/custom/vits-male
 69: tts_models/bn/custom/vits-female
 70: tts_models/be/common-voice/glow-tts

 Name format: type/language/dataset/model
 1: vocoder_models/universal/libri-tts/wavegrad
 2: vocoder_models/universal/libri-tts/fullband-melgan
 3: vocoder_models/en/ek1/wavegrad
 4: vocoder_models/en/ljspeech/multiband-melgan [already downloaded]
 5: vocoder_models/en/ljspeech/hifigan_v2
 6: vocoder_models/en/ljspeech/univnet
 7: vocoder_models/en/blizzard2013/hifigan_v2
 8: vocoder_models/en/vctk/hifigan_v2
 9: vocoder_models/en/sam/hifigan_v2
 10: vocoder_models/nl/mai/parallel-wavegan
 11: vocoder_models/de/thorsten/wavegrad
 12: vocoder_models/de/thorsten/fullband-melgan
 13: vocoder_models/de/thorsten/hifigan_v1
 14: vocoder_models/ja/kokoro/hifigan_v1
 15: vocoder_models/uk/mai/multiband-melgan
 16: vocoder_models/tr/common-voice/hifigan
 17: vocoder_models/be/common-voice/hifigan

 Name format: type/language/dataset/model
 1: voice_conversion_models/multilingual/vctk/freevc24
(venv) hazem-elbatawy@hazem-elbatawy-Vostro-15-3510:
~/Downloads/vox-estate-agent/backend/realestate_agent$ 



answer
---------
Summarized conversation historyYou have successfully listed all available
 Coqui TTS models in your virtual environment. For Arabic TTS, none of the 
 models in the list are explicitly labeled for Arabic (ar), but you do have
  several multilingual models available:
-------------------------------------------

- `tts_models/multilingual/multi-dataset/xtts_v2`
- `tts_models/multilingual/multi-dataset/xtts_v1.1`
- `tts_models/multilingual/multi-dataset/your_tts`
- `tts_models/multilingual/multi-dataset/bark`

These multilingual models (especially XTTS v2) support Arabic text synthesis. 
You should use one of these for Arabic TTS.

Next steps:
--------------

1. Update your backend TTS logic to use `tts_models/multilingual/multi-dataset/xtts_v2` 
for Arabic.
2. Test generating Arabic speech using this model.

Would you like help updating your backend code to use XTTS v2 for Arabic TTS?
yes



This is my current snippet of code for 
backend/realestate_agent/tts/tts_coqui.py


# Basic TTS module using Coqui TTS
# Install with: pip install TTS numpy scipy
from TTS.api import TTS
import numpy as np
from scipy.io.wavfile import write
from typing import Union, List

def generate_tts_audio(text: Union[str, List[str]], output_path: str = "tts_output.wav") -> str:
    import time
    # Use a more reliable model (glow-tts)
    tts = TTS(model_name="tts_models/en/ljspeech/glow-tts", progress_bar=False, gpu=False)
    if isinstance(text, str):
        texts = [text]
    else:
        texts = list(text)
    # Generate audio for all sentences by looping
    wavs = [tts.tts(sentence) for sentence in texts]
    # Concatenate all audio arrays
    sample_rate = tts.synthesizer.output_sample_rate
    combined = np.concatenate(wavs)
    # Generate timestamped filename
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_path = f"tts_output_{timestamp}.wav"
    # Normalize audio to -1.0 to 1.0 range before converting to int16
    max_val = np.max(np.abs(combined))
    if max_val > 0:
        combined = combined / max_val
    write(output_path, sample_rate, (combined * 32767).astype(np.int16))
    return output_path

if __name__ == "__main__":
    test_texts = [
        "Hello, this is a test of the Coqui TTS system.",
        "This demonstrates batching multiple sentences.",
        "All sentences will be combined into one audio file."
    ]
    audio_file = generate_tts_audio(test_texts)
    print(f"Audio saved to {audio_file}")
 
 ** update it by your code and provide me the full code snippet



 #####################################

                        Arabic tts update code  
                                                #####################################

# Basic TTS module using Coqui TTS XTTS v2 (multilingual, supports Arabic)
# Install with: pip install TTS numpy scipy
from TTS.api import TTS
import numpy as np
from scipy.io.wavfile import write
from typing import Union, List

def generate_tts_audio(
    text: Union[str, List[str]],
    output_path: str = "tts_output.wav",
    language: str = "en"
) -> str:
    import time
    # Use XTTS v2 (multilingual, supports Arabic)
    tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False, gpu=False)
    if isinstance(text, str):
        texts = [text]
    else:
        texts = list(text)
    # Generate audio for all sentences by looping
    wavs = [tts.tts(sentence, language=language) for sentence in texts]
    # Concatenate all audio arrays
    sample_rate = tts.synthesizer.output_sample_rate
    combined = np.concatenate(wavs)
    # Generate timestamped filename
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_path = f"tts_output_{timestamp}.wav"
    # Normalize audio to -1.0 to 1.0 range before converting to int16
    max_val = np.max(np.abs(combined))
    if max_val > 0:
        combined = combined / max_val
    write(output_path, sample_rate, (combined * 32767).astype(np.int16))
    return output_path

if __name__ == "__main__":
    test_texts = [
        "Hello, this is a test of the Coqui TTS system.",
        "This demonstrates batching multiple sentences.",
        "All sentences will be combined into one audio file.",
        "مرحبا! هذا اختبار للنظام باللغة العربية."  # Arabic test
    ]
    # English example
    audio_file_en = generate_tts_audio(test_texts[:3], language="en")
    print(f"English audio saved to {audio_file_en}")
    # Arabic example
    audio_file_ar = generate_tts_audio(test_texts[3], language="ar")
    print(f"Arabic audio saved to {audio_file_ar}")

    ````

    Set language="ar" for Arabic, language="en" for English, etc.
XTTS v2 supports many languages, including Arabic.
You can batch sentences and mix languages as needed.
Let me know if you want auto-detection of language or further customization!



#########################################################################3



 > You must confirm the following:
 | > "I have purchased a commercial license from Coqui: licensing@coqui.ai"
 | > "Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml" - [y/n]
 | | > y
 > Downloading model to /home/hazem-elbatawy/snap/code/206/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2


Answer:
-------
You have confirmed the Coqui license terms and XTTS v2 is now downloading. Once the download completes, your updated backend code will be able to generate Arabic speech using XTTS v2.

Next steps:
- Wait for the model download to finish.
- Test your updated `tts_coqui.py` with Arabic text (set `language="ar"`).
- If you encounter any issues or want to add auto language detection, let me know!

Your backend is now ready for multilingual TTS, including Arabic.





#################################################

To check how much of the XTTS v2 model has downloaded:

1. Open a terminal.
2. Navigate to the model directory:
   ```bash
   cd /home/hazem-elbatawy/snap/code/206/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2
   ```
3. List files and their sizes:
   ```bash
   ls -lh
   ```
   or for total size:
   ```bash
   du -sh .
   ```

This will show you the current downloaded size and files. If the download is still in progress, you’ll see files growing in size or new files appearing.



bash

(venv) hazem-elbatawy@hazem-elbatawy-Vostro-15-3510:~/snap/code/206/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2$ ls -lh
total 1.3G
-rw-rw-r-- 1 hazem-elbatawy hazem-elbatawy 1.3G Sep 16 23:33 model.pth
-rw-rw-r-- 1 hazem-elbatawy hazem-elbatawy   63 Sep 16 21:57 tos_agreed.txt
(venv) hazem-elbatawy@hazem-elbatawy-Vostro-15-3510:~/snap/code/206/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2$ du -sh .
1.3G    .
(venv) hazem-elbatawy@hazem-elbatawy-Vostro-15-3510:~/snap/code/206/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2$ 


Answer
----------
Your XTTS v2 model has downloaded about 1.3 GB so far (`model.pth`). The download is still in progress or may be nearly complete, depending on the total model size (often 2–4 GB).

If the file size keeps increasing, the download is ongoing. Once it stops growing and no new files appear, the model is ready for use.

You can keep checking with `ls -lh` and `du -sh .` to monitor progress. Let me know if you need help
 testing TTS once the download finishes!


 
