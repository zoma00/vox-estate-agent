VPS Basic — Quickstart for PropEstateAI

This file contains a concise, copy-pasteable quickstart for deploying PropEstateAI on a small VPS (suitable for the $5.99/month 1 vCPU / 1GB / 30GB plan for demo use). It covers bootstrap steps, safe deploy practices for low-memory systems, nginx + certbot, swap, and upgrade guidance.

1. Before you start
- Use local builds for frontends (do not build on the VPS). This saves memory and CPU.
- Exclude large generated media (audio/video) from the deploy package. Use S3 or provider object storage for media.
- Plan to enable swap (1-2GB) on the VPS and monitor CPU/memory closely.

2. Initial server bootstrap (run as your SSH user)
```bash
# update + upgrade
sudo apt update && sudo apt upgrade -y

# create deploy user (optional)
sudo adduser deploy
sudo usermod -aG sudo deploy

# install essentials
sudo apt install -y nginx git python3 python3-venv python3-pip nodejs npm \
  certbot python3-certbot-nginx ufw

# firewall
sudo ufw allow OpenSSH
sudo ufw allow 'Nginx Full'
sudo ufw enable
```

3. Set up swap (emergency fallback)
```bash
# create 1GB swapfile
sudo fallocate -l 1G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
# persist
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

4. Prepare deploy artifact locally (recommended)
- Build your frontends locally:
  - `cd web-frontend/webfront && npm ci && npm run build`
  - `cd mobile-frontend && npm ci && npm run build`
- Create a tarball excluding large media and dev files:
```bash
cat > deploy_exclude.txt <<EOF
backend/results/
backend/tts_output*.wav
backend/media/
**/*.log
node_modules/
mobile-frontend/dist/
web-frontend/webfront/build/
.vscode/
.env
EOF

tar --exclude-from=deploy_exclude.txt -czf propestateai-deploy.tar.gz backend web-frontend mobile-frontend package.json requirements.txt scripts deploy/
```

5. Upload and extract on VPS
```bash
scp propestateai-deploy.tar.gz deploy@VPS_IP:/home/deploy/
ssh deploy@VPS_IP
mkdir -p /home/deploy/propestateai
tar -xzf propestateai-deploy.tar.gz -C /home/deploy/propestateai
```

6. Backend virtualenv and install (on VPS)
```bash
cd /home/deploy/propestateai/backend/realestate_agent
python3 -m venv venv
source venv/bin/activate
pip install --no-cache-dir -r requirements.txt
```

7. Create systemd service (example `/etc/systemd/system/propestateai.service`)
```ini
[Unit]
Description=PropEstateAI FastAPI app (uvicorn)
After=network.target

[Service]
User=deploy
Group=www-data
WorkingDirectory=/home/deploy/propestateai/backend/realestate_agent
EnvironmentFile=/etc/propestateai.env
ExecStart=/home/deploy/propestateai/backend/realestate_agent/venv/bin/uvicorn realestate_agent.main:app --host 127.0.0.1 --port 8000 --workers 1
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

8. Nginx configuration (example `/etc/nginx/sites-available/propestateai`)
```nginx
server {
  listen 80;
  server_name propestateai.site www.propestateai.site;
  return 301 https://$host$request_uri;
}

server {
  listen 443 ssl;
  server_name propestateai.site www.propestateai.site;

  ssl_certificate /etc/letsencrypt/live/propestateai.site/fullchain.pem;
  ssl_certificate_key /etc/letsencrypt/live/propestateai.site/privkey.pem;
  include /etc/letsencrypt/options-ssl-nginx.conf;
  ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem;

  root /var/www/propestateai/web;
  index index.html;

  location / {
    try_files $uri /index.html;
  }

  location /mobile/ {
    alias /var/www/propestateai/mobile/;
    try_files $uri /mobile/index.html;
  }

  location /api/ {
    proxy_pass http://127.0.0.1:8000/;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
  }
}
```

9. Obtain TLS with Certbot
```bash
sudo certbot --nginx -d propestateai.site -d www.propestateai.site
sudo certbot renew --dry-run
```

10. Post-deploy checks and monitoring
- Visit `https://propestateai.site` and `https://propestateai.site/mobile/`
- Check backend: `curl http://127.0.0.1:8000/health` (or root)
- Monitor: `sudo journalctl -u propestateai.service -f` and `sudo tail -f /var/log/nginx/error.log`
- Install Netdata or rely on provider metrics to watch CPU and RAM.

11. Upgrade path
- If you see memory or CPU pressure, upgrade to 2GB/2vCPU. If TTS or heavier tasks are needed, use separate worker instances or GPU-backed nodes.

12. Misc notes
- Use managed Postgres (you said you'll connect to pg4admin) and store media in object storage.
- Add generated media patterns to `.gitignore` to avoid future commits.

---

If you want I can now create `deploy/nginx/propestateai.conf` and `/deploy/systemd/propestateai.service` files in the repo prefilled with your `VPS_IP` and `deploy` username and commit them. Which would you like me to do next?



#####################################

                    Basic and TTS ollama local

                    ##########################################

Short answer
- If your backend only calls the OpenAI API (remote inference), the cheap VPS (1 vCPU / 1 GB) can run uvicorn + FastAPI for a lightweight demo with very low concurrency — but it will be fragile under load and slow for concurrent TTS requests.
- If you want to run local model inference (Ollama, local LLMs, or heavy TTS like ttspyx3 if it runs locally), the cheap VPS will usually NOT be sufficient. Local models need much more RAM/CPU (often 8–16+ GB or GPU) depending on model size.

Explanation and concrete guidance

1) OpenAI API (remote) — what happens and feasibility
- Behavior: your FastAPI app receives HTTP from React and forwards requests to OpenAI (text or TTS endpoints), receives the response (audio/JSON), and returns it to the client or saves it.
- Resource usage on VPS:
  - CPU: low per-request (HTTP client work, JSON parsing). 1 vCPU is okay for a demo but will become CPU-bound when several requests come in.
  - RAM: uvicorn + app + libraries typically < 200–400MB for tiny apps — fits in 1 GB if you keep worker count low.
  - Network: you will use bandwidth for audio payloads; OpenAI costs are separate.
- Recommendations for stable behavior on 1 GB VPS:
  - Use a single uvicorn worker (do not spawn many workers). Example systemd ExecStart uses `--workers 1`.
  - Use an async HTTP client (httpx/asyncio) so requests don’t block the event loop.
  - Put long-running request handling in background tasks or queue jobs (so the HTTP response can be quick or polled).
  - Limit concurrency with a simple semaphore or rate limiter.
  - Add swap (1–2 GB) as emergency fallback (not a substitute for RAM).
- Bottom line: OpenAI integration is feasible on cheap VPS for demos, but scale/latency and reliability are limited.

2) Local TTS engine (ttspyx3 or similar) or Ollama local models — why they need more resources
- Local TTS engines and model inference typically:
  - consume substantial RAM (models can be hundreds of MBs to many GBs),
  - use many CPU cycles (or need GPU for acceptable latency),
  - may store large model files on disk (several GB).
- Ollama (local LLM runner) specifics:
  - Ollama runs models locally and serves via a local HTTP API. Small quantized models might be runnable in ~8GB RAM (sometimes less for very small models), but mainstream LLMs (7B, 13B) commonly need 8–16GB or more; inference latency on CPU can be high.
  - Recommended: run Ollama on a machine with at least 8–16GB RAM; GPU is strongly recommended for larger models or low-latency inference.
- If ttspyx3 is a local TTS model (native inference), similar constraints apply — it will likely not be practical on a 1GB VPS.

3) Practical architectures you can use (recommended)
- Option A — Cheap VPS + OpenAI remote TTS (recommended for demo)
  - Keep the backend on the 1GB VPS; forward TTS/LLM requests to OpenAI (or other hosted endpoints). Minimal resource usage on VPS.
  - Use async calls and background tasks to avoid blocking.
- Option B — Cheap VPS + external Ollama / inference server (best for hybrid)
  - Run Ollama (or heavy TTS) on a separate machine (e.g., a desktop with 16GB, a cloud VM with 16GB/GPUs) and call it from the 1GB VPS backend via private IP or secure tunnel.
  - This keeps the cheap VPS as the web & API orchestrator, while heavy inference runs elsewhere.
- Option C — Single powerful VPS / instance (if you must have everything on one host)
  - Choose at least 8–16 GB RAM and multiple vCPUs, or a GPU instance for real-time inference. This is more expensive but simpler.

4) Concrete configuration & server-side patterns (what to change on your app)
- Use async HTTP client and small worker count
  - Example using httpx in FastAPI:
```python
import httpx
from fastapi import FastAPI, BackgroundTasks

app = FastAPI()

async def forward_to_openai(payload):
    async with httpx.AsyncClient(timeout=60) as client:
        resp = await client.post("https://api.openai.com/v1/...", json=payload, headers={"Authorization": f"Bearer {OPENAI_KEY}"})
        return resp.json()

@app.post("/api/tts")
async def tts_endpoint(payload: dict, background_tasks: BackgroundTasks):
    # fire background job if you want non-blocking
    background_tasks.add_task(forward_to_openai, payload)
    return {"status":"queued"}  # or return the result if you await forward_to_openai directly
```
- Limit concurrency (simple semaphore)
```python
import asyncio
sem = asyncio.Semaphore(4)  # limit concurrent external calls

async def safe_call(...):
    async with sem:
        ...
```
- Use a job queue for heavy tasks (recommended)
  - RQ, Dramatiq, or Celery with Redis: the 1GB VPS runs the web API, and jobs are processed by worker(s) on another machine (or scaled later).
- Uvicorn parameters
  - For 1GB plan: use `--workers 1` and tune timeout. In systemd service set `--workers 1` and rely on async concurrency.
- Connection pooling and timeouts
  - Use a single AsyncClient for reuse or create per-request with proper timeouts. Avoid blocking synchronous calls.

5) Resource recommendations
- For reliable production with OpenAI-only: 2 vCPU / 4 GB RAM is a good baseline.
- For running small local Ollama model (very small quantized) or lightweight TTS: min 8 GB RAM.
- For comfortable LLM inference or real-time TTS: 16+ GB RAM or GPU instances.

6) Edge cases & operational cautions
- Rate limits: OpenAI rate limits can cause long waits; queue requests.
- Cost: OpenAI TTS/LLM costs may be significant — watch usage.
- Latency: remote calls add latency; if you need real-time interactive TTS, local inference or a fast inference server is better.
- Security: never expose model server ports publicly without authentication.

7) Next actionable options I can help with (pick one)
- Add a background job + RQ worker pattern and a sample systemd unit for a worker you can run on another host.
- Add async httpx code and concurrency controls to your existing FastAPI endpoints (I can edit example files).
- Draft a minimal Ollama deployment checklist with recommended VM specs (8/16/32 GB and GPU notes).
- Prepare a cheap test plan: run a load test (k6 or hey) to measure how many concurrent OpenAI-forwarding requests your 1GB VPS can sustain with `--workers 1` and async io.

Which would you like me to implement now? If you want me to edit code, tell me the backend file to edit (e.g., path to your FastAPI endpoint) and I’ll add an async-forwarding example + semaphore + background tasks.








Local repo disk-usage (from your machine)
```
```bash
du -sh /home/hazem-elbatawy/Downloads/vox-estate-agent
8.4G    /home/hazem-elbatawy/Downloads/vox-estate-agent
du -h --max-depth=1 /home/hazem-elbatawy/Downloads/vox-estate-agent | sort -h
8.0K    /home/hazem-elbatawy/Downloads/vox-estate-agent/.githooks
12K     /home/hazem-elbatawy/Downloads/vox-estate-agent/scripts
12K     /home/hazem-elbatawy/Downloads/vox-estate-agent/tools
12K     /home/hazem-elbatawy/Downloads/vox-estate-agent/UI changes both gallery 
20K     /home/hazem-elbatawy/Downloads/vox-estate-agent/deploy
36K     /home/hazem-elbatawy/Downloads/vox-estate-agent/docs
40K     /home/hazem-elbatawy/Downloads/vox-estate-agent/Server config
6.1M    /home/hazem-elbatawy/Downloads/vox-estate-agent/Tips
13M     /home/hazem-elbatawy/Downloads/vox-estate-agent/node_modules
46M     /home/hazem-elbatawy/Downloads/vox-estate-agent/.git
48M     /home/hazem-elbatawy/Downloads/vox-estate-agent/mobile-frontend
488M    /home/hazem-elbatawy/Downloads/vox-estate-agent/web-frontend
7.8G    /home/hazem-elbatawy/Downloads/vox-estate-agent/backend
8.4G    /home/hazem-elbatawy/Downloads/vox-estate-agent
```












